{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false
            },
            "source": [
                "# Simulations with reinforcement learning"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "In this notebook you can run simulations with 5 different reinforcement learning algorithms. There are different example buildings that can be simulated.\n",
                "\n",
                "<br>\n",
                "\n",
                "Things to focus on:\n",
                "-  Why is it making more sub_runs than episodes??\n",
                "-  There are only 10 episodes sub-files that stay. The rest is deleted??\n",
                "-  Better to make a list with the mean rewards, so we can make plots after running.\n",
                "-  Now, the room is always on temperature, how to see this. Weekends off? only on on working hours? \n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sinergym\n",
                "from sinergym.utils.callbacks import LoggerEvalCallback, LoggerCallback\n",
                "from sinergym.utils.rewards import *\n",
                "from sinergym.utils.wrappers import LoggerWrapper\n",
                "from sinergym.utils.constants import RANGES_5ZONE\n",
                "from datetime import datetime\n",
                "import gym\n",
                "from stable_baselines3 import DQN, DDPG, PPO, A2C, SAC, TD3 \n",
                "\n",
                "from stable_baselines3.common.logger import configure\n",
                "\n",
                "from stable_baselines3.common.callbacks import CallbackList\n",
                "from stable_baselines3.common.vec_env import DummyVecEnv\n",
                "import numpy as np\n",
                "import mlflow"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next you can set different variables for het simulation. You can choose the period that you want to simulate.\n",
                "\n",
                "(For now the year is always 1991, has something to do with the current weather file, which is standard in the environment. Can be changed later on)\n",
                "\n",
                "<br>\n",
                "\n",
                "You also set the reward function here. Now it is set to exponential.\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-01-11 10:19:38,315] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
                        "[2023-01-11 10:19:38,315] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
                        "[2023-01-11 10:19:38,326] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
                        "[2023-01-11 10:19:38,326] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
                        "[2023-01-11 10:19:38,343] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
                        "[2023-01-11 10:19:38,343] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
                        "[2023-01-11 10:19:38,357] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
                        "[2023-01-11 10:19:38,357] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
                        "[2023-01-11 10:19:38,366] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n",
                        "[2023-01-11 10:19:38,366] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
                        "  logger.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-01-11 10:19:38,973] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:19:38,973] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:19:39,209] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-continuous-v1-res56/Eplus-env-sub_run1\n",
                        "[2023-01-11 10:19:39,209] EPLUS_ENV_5Zone-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-continuous-v1-res56/Eplus-env-sub_run1\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [4], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m callback \u001b[39m=\u001b[39m CallbackList(callbacks)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m     60\u001b[0m     total_timesteps\u001b[39m=\u001b[39;49mtimesteps,\n\u001b[1;32m     61\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m     62\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     63\u001b[0m model\u001b[39m.\u001b[39msave(env\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39m_env_working_dir_parent \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name)\n\u001b[1;32m     65\u001b[0m \u001b[39m# End mlflow run\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    318\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    319\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    320\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    321\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    322\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    323\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    324\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    325\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    326\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    327\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    328\u001b[0m     )\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    179\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 181\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    185\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
                        "File \u001b[0;32m/sinergym/sinergym/utils/wrappers.py:194\u001b[0m, in \u001b[0;36mLoggerWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Union[\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mndarray]\n\u001b[1;32m    185\u001b[0m          ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    186\u001b[0m     \u001b[39m\"\"\"Step the environment. Logging new information\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m        Tuple[np.ndarray, float, bool, Dict[str, Any]]: Tuple with next observation, reward, bool for terminated episode and dict with extra information.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    195\u001b[0m     \u001b[39m# We added some extra values (month,day,hour) manually in env, so we\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# need to delete them.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m is_wrapped(\u001b[39mself\u001b[39m, NormalizeObservation):\n\u001b[1;32m    198\u001b[0m         \u001b[39m# Record action and new observation in simulator's csv\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
                        "File \u001b[0;32m/sinergym/sinergym/envs/eplus_env.py:197\u001b[0m, in \u001b[0;36mEplusEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39mlogger_main\u001b[39m.\u001b[39mdebug(action_)\n\u001b[1;32m    196\u001b[0m \u001b[39m# time_info = (current simulation year, month, day, hour, time_elapsed)\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m time_elapsed, obs, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimulator\u001b[39m.\u001b[39;49mstep(action_)\n\u001b[1;32m    198\u001b[0m \u001b[39m# Create dictionary with observation\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m], obs))\n",
                        "File \u001b[0;32m/sinergym/sinergym/simulators/eplus.py:282\u001b[0m, in \u001b[0;36mEnergyPlus.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conn\u001b[39m.\u001b[39msend(tosend\u001b[39m.\u001b[39mencode())\n\u001b[1;32m    281\u001b[0m \u001b[39m# Recieve from EnergyPlus\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m rcv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrecv(\u001b[39m2048\u001b[39;49m)\u001b[39m.\u001b[39mdecode(encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger_main\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mGot message successfully: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m rcv)\n\u001b[1;32m    284\u001b[0m \u001b[39m# Process received msg\u001b[39;00m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "#environment = \"Eplus-demo-v1\"\n",
                "environment  = \"Eplus-5Zone-hot-continuous-v1\"\n",
                "#environment = \"Eplus-5Zone-mixed- continuous-v1\"\n",
                "\n",
                "\n",
                "\n",
                "episodes = 1\n",
                "experiment_date = datetime.today().strftime('%Y-%m-%d %H:%M')\n",
                "\n",
                "#choose the simulation period\n",
                "begin_day = 1\n",
                "begin_month = 1\n",
                "begin_year = 2022\n",
                "end_day = 1\n",
                "end_month = 2\n",
                "end_year = 2022\n",
                "\n",
                "# register run name\n",
                "name = F\"{environment}-episodes_{episodes}({experiment_date})\"\n",
                "\n",
                "\n",
                "# Set to one month only to reduce running time\n",
                "extra_params={'timesteps_per_hour' : 4,\n",
                "              'runperiod' : (begin_day,begin_month,begin_year,end_day,end_month,end_year)}\n",
                "\n",
                "with mlflow.start_run(run_name=name):\n",
                "    env = gym.make(environment, reward=LinearReward)\n",
                "    env = LoggerWrapper(env)\n",
                "\n",
                "    # Defining model(algorithm)\n",
                "    model = PPO('MlpPolicy', env)\n",
                "\n",
                "    # Calculating n_timesteps_episode for training\n",
                "    n_timesteps_episode = env.simulator._eplus_one_epi_len / \\\n",
                "                          env.simulator._eplus_run_stepsize\n",
                "    timesteps = episodes * n_timesteps_episode\n",
                "\n",
                "    # For callbacks processing\n",
                "    env_vec = DummyVecEnv([lambda: env])\n",
                "\n",
                "    # Using Callbacks for training\n",
                "    callbacks = []\n",
                "\n",
                "    # Set up Evaluation and saving best model\n",
                "    eval_callback = LoggerEvalCallback(\n",
                "        env_vec,\n",
                "        best_model_save_path='best_model/' + name + '/',\n",
                "        log_path='best_model/' + name + '/',\n",
                "        eval_freq=n_timesteps_episode * 2,\n",
                "        deterministic=True,\n",
                "        render=False,\n",
                "        n_eval_episodes=2)\n",
                "    callbacks.append(eval_callback)\n",
                "\n",
                "    callback = CallbackList(callbacks)\n",
                "\n",
                "    # Training\n",
                "    model.learn(\n",
                "        total_timesteps=timesteps,\n",
                "        callback=callback,\n",
                "        log_interval=1)\n",
                "    model.save(env.simulator._env_working_dir_parent + '/' + name)\n",
                "\n",
                "    # End mlflow run\n",
                "    mlflow.end_run()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "# schedulers=env.get_schedulers()\n",
                "# print(schedulers)\n",
                "# actuators=env.get_schedulers(path='./example.xlsx')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tenserflow check\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-01-11 10:30:38,092] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
                        "[2023-01-11 10:30:38,092] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
                        "[2023-01-11 10:30:38,092] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf ExternalInterface object if it is not present...\n",
                        "[2023-01-11 10:30:38,101] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
                        "[2023-01-11 10:30:38,101] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
                        "[2023-01-11 10:30:38,101] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
                        "[2023-01-11 10:30:38,123] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
                        "[2023-01-11 10:30:38,123] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
                        "[2023-01-11 10:30:38,123] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Updating idf OutPut:Variable and variables XML tree model for BVCTB connection.\n",
                        "[2023-01-11 10:30:38,144] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
                        "[2023-01-11 10:30:38,144] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
                        "[2023-01-11 10:30:38,144] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
                        "[2023-01-11 10:30:38,149] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n",
                        "[2023-01-11 10:30:38,149] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n",
                        "[2023-01-11 10:30:38,149] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n"
                    ]
                }
            ],
            "source": [
                "environment = \"Eplus-demo-v1\"\n",
                "episodes = 4\n",
                "experiment_date = datetime.today().strftime('%Y-%m-%d %H:%M')\n",
                "\n",
                "# register run name\n",
                "name = F\"DQN-{environment}-episodes_{episodes}({experiment_date})\"\n",
                "\n",
                "env = gym.make(environment, reward=LinearReward)\n",
                "env = LoggerWrapper(env)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cpu device\n",
                        "Wrapping the env with a `Monitor` wrapper\n",
                        "Wrapping the env in a DummyVecEnv.\n"
                    ]
                }
            ],
            "source": [
                "tensorboard_path='./tensorboard_log'\n",
                "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=tensorboard_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_timesteps_episode = env.simulator._eplus_one_epi_len / \\\n",
                "                      env.simulator._eplus_run_stepsize\n",
                "timesteps = episodes * n_timesteps_episode\n",
                "env_vec = DummyVecEnv([lambda: env])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-01-11 10:30:40.014335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2023-01-11 10:30:48.982620: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2023-01-11 10:30:59.368498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-01-11 10:30:59.369628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-01-11 10:30:59.369730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
                    ]
                }
            ],
            "source": [
                "callbacks = []\n",
                "\n",
                "# Set up Evaluation and saving best model\n",
                "log_callback = LoggerCallback(True)\n",
                "callbacks.append(log_callback)\n",
                "# lets change default dir for TensorboardFormatLogger only\n",
                "tb_path = tensorboard_path + '/' + name\n",
                "new_logger = configure(tb_path, [\"tensorboard\"])\n",
                "model.set_logger(new_logger)\n",
                "\n",
                "callback = CallbackList(callbacks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-01-11 10:31:06,741] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:31:06,741] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:31:06,741] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:31:06,957] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run1\n",
                        "[2023-01-11 10:31:06,957] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run1\n",
                        "[2023-01-11 10:31:06,957] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run1\n",
                        "[2023-01-11 10:38:44,232] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus episode completed successfully. \n",
                        "[2023-01-11 10:38:44,232] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus episode completed successfully. \n",
                        "[2023-01-11 10:38:44,232] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus episode completed successfully. \n",
                        "[2023-01-11 10:38:44,258] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:38:44,258] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:38:44,258] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-01-11 10:38:44,858] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run2\n",
                        "[2023-01-11 10:38:44,858] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run2\n",
                        "[2023-01-11 10:38:44,858] EPLUS_ENV_demo-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-demo-v1-res7/Eplus-env-sub_run2\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m      2\u001b[0m     total_timesteps\u001b[39m=\u001b[39;49mtimesteps,\n\u001b[1;32m      3\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m      4\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39msave(env\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39m_env_working_dir_parent \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name)\n\u001b[1;32m      6\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/dqn/dqn.py:275\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m: DQNSelf,\n\u001b[1;32m    263\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DQNSelf:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    276\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    277\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    278\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    279\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    280\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    281\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    282\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    283\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    284\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    285\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    286\u001b[0m     )\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py:356\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    353\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    355\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 356\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    357\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    358\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    359\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    360\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    361\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    362\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    363\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    366\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py:589\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    586\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    588\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 589\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    591\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    592\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
                        "File \u001b[0;32m/sinergym/sinergym/utils/wrappers.py:194\u001b[0m, in \u001b[0;36mLoggerWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Union[\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mndarray]\n\u001b[1;32m    185\u001b[0m          ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    186\u001b[0m     \u001b[39m\"\"\"Step the environment. Logging new information\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m        Tuple[np.ndarray, float, bool, Dict[str, Any]]: Tuple with next observation, reward, bool for terminated episode and dict with extra information.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    195\u001b[0m     \u001b[39m# We added some extra values (month,day,hour) manually in env, so we\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# need to delete them.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m is_wrapped(\u001b[39mself\u001b[39m, NormalizeObservation):\n\u001b[1;32m    198\u001b[0m         \u001b[39m# Record action and new observation in simulator's csv\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
                        "File \u001b[0;32m/sinergym/sinergym/envs/eplus_env.py:197\u001b[0m, in \u001b[0;36mEplusEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39mlogger_main\u001b[39m.\u001b[39mdebug(action_)\n\u001b[1;32m    196\u001b[0m \u001b[39m# time_info = (current simulation year, month, day, hour, time_elapsed)\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m time_elapsed, obs, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimulator\u001b[39m.\u001b[39;49mstep(action_)\n\u001b[1;32m    198\u001b[0m \u001b[39m# Create dictionary with observation\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m], obs))\n",
                        "File \u001b[0;32m/sinergym/sinergym/simulators/eplus.py:282\u001b[0m, in \u001b[0;36mEnergyPlus.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conn\u001b[39m.\u001b[39msend(tosend\u001b[39m.\u001b[39mencode())\n\u001b[1;32m    281\u001b[0m \u001b[39m# Recieve from EnergyPlus\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m rcv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrecv(\u001b[39m2048\u001b[39;49m)\u001b[39m.\u001b[39mdecode(encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger_main\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mGot message successfully: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m rcv)\n\u001b[1;32m    284\u001b[0m \u001b[39m# Process received msg\u001b[39;00m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "model.learn(\n",
                "    total_timesteps=timesteps,\n",
                "    callback=callback,\n",
                "    log_interval=1)\n",
                "model.save(env.simulator._env_working_dir_parent + '/' + name)\n",
                "env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.6 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
