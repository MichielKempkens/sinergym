{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false
            },
            "source": [
                "# Simulations with reinforcement learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sinergym\n",
                "from sinergym.utils.callbacks import LoggerEvalCallback\n",
                "from sinergym.utils.rewards import *\n",
                "from datetime import datetime\n",
                "import gym\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "from sinergym.utils.wrappers import LoggerWrapper, NormalizeObservation\n",
                "from sinergym.utils.constants import RANGES_5ZONE\n",
                "\n",
                "from stable_baselines3 import DQN, DDPG, PPO, A2C, SAC, TD3\n",
                "\n",
                "\n",
                "from stable_baselines3.common.callbacks import CallbackList\n",
                "from stable_baselines3.common.vec_env import DummyVecEnv\n",
                "from stable_baselines3.common.logger import configure\n",
                "\n",
                "from math import exp\n",
                "from typing import Any, Dict, List, Tuple, Union\n",
                "from gym import Env\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Implementation of reward functions.\"\"\"\n",
                "\n",
                "\n",
                "class BaseReward(object):\n",
                "\n",
                "    def __init__(self, env):\n",
                "        \"\"\"\n",
                "        Base reward class.\n",
                "\n",
                "        All reward functions should inherit from this class.\n",
                "\n",
                "        Args:\n",
                "            env (Env): Gym environment.\n",
                "        \"\"\"\n",
                "        self.env = env\n",
                "\n",
                "    def __call__(self):\n",
                "        \"\"\"Method for calculating the reward function.\"\"\"\n",
                "        raise NotImplementedError(\n",
                "            \"Reward class must have a `__call__` method.\")\n",
                "\n",
                "\n",
                "class MyLinearReward(BaseReward):\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        env: Env,\n",
                "        temperature_variable: Union[str, list],\n",
                "        energy_variable: str,\n",
                "        range_comfort_winter: Tuple[int, int],\n",
                "        range_comfort_summer: Tuple[int, int],\n",
                "        summer_start: Tuple[int, int] = (6, 1),\n",
                "        summer_final: Tuple[int, int] = (9, 30),\n",
                "        energy_weight: float = 0.6,\n",
                "        #energy_weight: float = 1,\n",
                "        lambda_energy: float = 0.003,\n",
                "        #lambda_energy: float = 1,\n",
                "        lambda_temperature: float = 50\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Linear reward function.\n",
                "\n",
                "        It considers the energy consumption and the absolute difference to temperature comfort.\n",
                "\n",
                "        .. math::\n",
                "            R = - W * lambda_E * power - (1 - W) * lambda_T * (max(T - T_{low}, 0) + max(T_{up} - T, 0))\n",
                "\n",
                "        Args:\n",
                "            env (Env): Gym environment.\n",
                "            temperature_variable (Union[str, list]): Name(s) of the temperature variable(s).\n",
                "            energy_variable (str): Name of the energy/power variable.\n",
                "            range_comfort_winter (Tuple[int,int]): Temperature comfort range for cold season. Depends on environment you are using.\n",
                "            range_comfort_summer (Tuple[int,int]): Temperature comfort range for hot season. Depends on environment you are using.\n",
                "            summer_start (Tuple[int,int]): Summer session tuple with month and day start. Defaults to (6,1).\n",
                "            summer_final (Tuple[int,int]): Summer session tuple with month and day end. defaults to (9,30).\n",
                "            energy_weight (float, optional): Weight given to the energy term. Defaults to 0.5.\n",
                "            lambda_energy (float, optional): Constant for removing dimensions from power(1/W). Defaults to 1e-4.\n",
                "            lambda_temperature (float, optional): Constant for removing dimensions from temperature(1/C). Defaults to 1.0.\n",
                "        \"\"\"\n",
                "\n",
                "        super(MyLinearReward, self).__init__(env)\n",
                "\n",
                "        # Name of the variables\n",
                "        self.temp_name = temperature_variable\n",
                "        self.energy_name = energy_variable\n",
                "\n",
                "        # Reward parameters\n",
                "        self.range_comfort_winter = range_comfort_winter\n",
                "        self.range_comfort_summer = range_comfort_summer\n",
                "        self.W_energy = energy_weight\n",
                "        self.lambda_energy = lambda_energy\n",
                "        self.lambda_temp = lambda_temperature\n",
                "\n",
                "        # Summer period\n",
                "        self.summer_start = summer_start  # (month,day)\n",
                "        self.summer_final = summer_final  # (month,day)\n",
                "\n",
                "    def __call__(self) -> Tuple[float, Dict[str, Any]]:\n",
                "        \"\"\"\n",
                "        Calculate the reward function.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, Dict[str, Any]]: Reward value and dictionary with their individual components.\n",
                "        \"\"\"\n",
                "        # Current observation\n",
                "        obs_dict = self.env.obs_dict.copy()\n",
                "\n",
                "        # Energy term\n",
                "        #reward_energy = - self.lambda_energy * obs_dict[self.energy_name]\n",
                "        reward_energy = - (self.lambda_energy * obs_dict[self.energy_name])\n",
                "\n",
                "        # Comfort\n",
                "        comfort, temps = self._get_comfort(obs_dict)\n",
                "\n",
                "        if comfort == 0:\n",
                "            reward_comfort = 5\n",
                "        else:\n",
                "            reward_comfort = - self.lambda_temp * comfort\n",
                "        # Weighted sum of both terms\n",
                "        reward = self.W_energy * reward_energy + \\\n",
                "            (1.0 - self.W_energy) * reward_comfort\n",
                "\n",
                "        reward_terms = {\n",
                "            'reward_energy': reward_energy,\n",
                "            'total_energy': obs_dict[self.energy_name],\n",
                "            'reward_comfort': reward_comfort,\n",
                "            'abs_comfort': comfort,\n",
                "            'temperatures': temps\n",
                "        }\n",
                "\n",
                "        return reward, reward_terms\n",
                "\n",
                "    def _get_comfort(self,\n",
                "                     obs_dict: Dict[str,\n",
                "                                    Any]) -> Tuple[float,\n",
                "                                                   List[float]]:\n",
                "        \"\"\"Calculate the comfort term of the reward.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, List[float]]: comfort penalty and List with temperatures used.\n",
                "        \"\"\"\n",
                "\n",
                "        hour = obs_dict[\"hour\"]\n",
                "        month = obs_dict['month']\n",
                "        day = obs_dict['day']\n",
                "        year = obs_dict['year']\n",
                "        current_dt = datetime(year, month, day)\n",
                "\n",
                "        # Periods\n",
                "        summer_start_date = datetime(\n",
                "            year,\n",
                "            self.summer_start[0],\n",
                "            self.summer_start[1])\n",
                "        summer_final_date = datetime(\n",
                "            year,\n",
                "            self.summer_final[0],\n",
                "            self.summer_final[1])\n",
                "\n",
                "        if current_dt >= summer_start_date and current_dt <= summer_final_date:\n",
                "            if hour not in range(8,19):\n",
                "                temp_range = (15,30)\n",
                "            else:\n",
                "                temp_range = self.range_comfort_summer \n",
                "        else:\n",
                "            if hour not in range(8,19):\n",
                "                temp_range = (15,30)\n",
                "            else:\n",
                "                temp_range = self.range_comfort_winter\n",
                "\n",
                "\n",
                "        temps = [v for k, v in obs_dict.items() if k in self.temp_name]\n",
                "        comfort = 0.0\n",
                "        for T in temps:\n",
                "            if T < temp_range[0] or T > temp_range[1]:\n",
                "                comfort += min(abs(temp_range[0] - T), abs(T - temp_range[1]))\n",
                "  \n",
                " \n",
                "\n",
                "        return comfort, temps\n",
                "\n",
                "\n",
                "class MyExpReward(MyLinearReward):\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        env: Env,\n",
                "        temperature_variable: Union[str, list],\n",
                "        energy_variable: str,\n",
                "        range_comfort_winter: Tuple[int, int],\n",
                "        range_comfort_summer: Tuple[int, int],\n",
                "        summer_start: Tuple[int, int] = (6, 1),\n",
                "        summer_final: Tuple[int, int] = (9, 30),\n",
                "        #changes from 0.5 to 0.7\n",
                "        energy_weight: float = 1,\n",
                "        lambda_energy: float = 1e-4,\n",
                "        #lambda_energy: float = 1,\n",
                "        lambda_temperature: float = 1\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Reward considering exponential absolute difference to temperature comfort.\n",
                "\n",
                "        .. math::\n",
                "            R = - W * lambda_E * power - (1 - W) * lambda_T * exp( (max(T - T_{low}, 0) + max(T_{up} - T, 0)) )\n",
                "\n",
                "        Args:\n",
                "            env (Env): Gym environment.\n",
                "            temperature_variable (Union[str, list]): Name(s) of the temperature variable(s).\n",
                "            energy_variable (str): Name of the energy/power variable.\n",
                "            range_comfort_winter (Tuple[int,int]): Temperature comfort range for cold season. Depends on environment you are using.\n",
                "            range_comfort_summer (Tuple[int,int]): Temperature comfort range for hot season. Depends on environment you are using.\n",
                "            summer_start (Tuple[int,int]): Summer session tuple with month and day start. Defaults to (6,1).\n",
                "            summer_final (Tuple[int,int]): Summer session tuple with month and day end. defaults to (9,30).\n",
                "            energy_weight (float, optional): Weight given to the energy term. Defaults to 0.5.\n",
                "            lambda_energy (float, optional): Constant for removing dimensions from power(1/W). Defaults to 1e-4.\n",
                "            lambda_temperature (float, optional): Constant for removing dimensions from temperature(1/C). Defaults to 1.0.\n",
                "        \"\"\"\n",
                "\n",
                "        super(MyExpReward, self).__init__(\n",
                "            env,\n",
                "            temperature_variable,\n",
                "            energy_variable,\n",
                "            range_comfort_winter,\n",
                "            range_comfort_summer,\n",
                "            summer_start,\n",
                "            summer_final,\n",
                "            energy_weight,\n",
                "            lambda_energy,\n",
                "            lambda_temperature\n",
                "        )\n",
                "\n",
                "    def _get_comfort(self,\n",
                "                     obs_dict: Dict[str,\n",
                "                                    Any]) -> Tuple[float,\n",
                "                                                   List[float]]:\n",
                "        \"\"\"Calculate the comfort term of the reward.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, List[float]]: comfort penalty and List with temperatures used.\n",
                "        \"\"\"\n",
                "\n",
                "        hour = obs_dict[\"hour\"]\n",
                "        month = obs_dict['month']\n",
                "        day = obs_dict['day']\n",
                "        year = obs_dict['year']\n",
                "        current_dt = datetime(year, month, day)\n",
                "\n",
                "        # Periods\n",
                "        summer_start_date = datetime(\n",
                "            year,\n",
                "            self.summer_start[0],\n",
                "            self.summer_start[1])\n",
                "        summer_final_date = datetime(\n",
                "            year,\n",
                "            self.summer_final[0],\n",
                "            self.summer_final[1])\n",
                "\n",
                "        # if current_dt >= summer_start_date and current_dt <= summer_final_date:\n",
                "        #     temp_range = self.range_comfort_summer \n",
                "        # else:\n",
                "        #     temp_range = self.range_comfort_winter\n",
                "        if current_dt >= summer_start_date and current_dt <= summer_final_date:\n",
                "            if current_dt.weekday() >= 5 or hour not in range(8,19):\n",
                "                temp_range = (15,30)\n",
                "            else:\n",
                "                temp_range = self.range_comfort_summer \n",
                "        else:\n",
                "            if current_dt.weekday() >= 5 or hour not in range(8,19):\n",
                "                temp_range = (15,30)\n",
                "            else:\n",
                "                temp_range = self.range_comfort_winter\n",
                "\n",
                "\n",
                "        temps = [v for k, v in obs_dict.items() if k in self.temp_name]\n",
                "        comfort = 0.0\n",
                "        for T in temps:\n",
                "            if T < temp_range[0] or T > temp_range[1]:\n",
                "                comfort += exp(min(abs(temp_range[0] - T),\n",
                "                                   abs(T - temp_range[1])))\n",
                "\n",
                "            # else:\n",
                "            #     comfort -= 5\n",
                "\n",
                "        return comfort, temps\n",
                "\n",
                "\n",
                "class MyHourlyExpReward(MyExpReward):\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        env: Env,\n",
                "        temperature_variable: Union[str, list],\n",
                "        energy_variable: str,\n",
                "        # range_comfort_winter: Tuple[int, int],\n",
                "        # range_comfort_summer: Tuple[int, int],\n",
                "        range_comfort_winter = (20,23),\n",
                "        range_comfort_summer = (23,26),\n",
                "        summer_start: Tuple[int, int] = (6, 1),\n",
                "        summer_final: Tuple[int, int] = (9, 30),\n",
                "        min_energy_weight: float = 0.6,\n",
                "        #default energy lambda = 1\n",
                "        lambda_energy: float = 0.001,\n",
                "        lambda_temperature: float = 1,\n",
                "        range_comfort_hours: tuple = (8, 19)\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Linear reward function with a time-dependent weight for consumption and energy terms.\n",
                "\n",
                "        Args:\n",
                "            env (Env): Gym environment.\n",
                "            temperature_variable (Union[str, list]): Name(s) of the temperature variable(s).\n",
                "            energy_variable (str): Name of the energy/power variable.\n",
                "            range_comfort_winter (Tuple[int,int]): Temperature comfort range for cold season. Depends on environment you are using.\n",
                "            range_comfort_summer (Tuple[int,int]): Temperature comfort range for hot season. Depends on environment you are using.\n",
                "            summer_start (Tuple[int,int]): Summer session tuple with month and day start. Defaults to (6,1).\n",
                "            summer_final (Tuple[int,int]): Summer session tuple with month and day end. defaults to (9,30).\n",
                "            min_energy_weight (float, optional): Minimum weight given to the energy term. Defaults to 0.5.\n",
                "            lambda_energy (float, optional): Constant for removing dimensions from power(1/W). Defaults to 1e-4.\n",
                "            lambda_temperature (float, optional): Constant for removing dimensions from temperature(1/C). Defaults to 1.0.\n",
                "            range_comfort_hours (tuple, optional): Hours where thermal comfort is considered. Defaults to (9, 19).\n",
                "        \"\"\"\n",
                "\n",
                "        super(MyHourlyExpReward, self).__init__(\n",
                "            env,\n",
                "            temperature_variable,\n",
                "            energy_variable,\n",
                "            range_comfort_winter,\n",
                "            range_comfort_summer,\n",
                "            summer_start,\n",
                "            summer_final,\n",
                "            min_energy_weight,\n",
                "            lambda_energy,\n",
                "            lambda_temperature\n",
                "        )\n",
                "\n",
                "\n",
                "\n",
                "        # Reward parameters\n",
                "        self.range_comfort_hours = range_comfort_hours\n",
                "\n",
                "    def __call__(self) -> Tuple[float, Dict[str, Any]]:\n",
                "        \"\"\"Calculate the reward function.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, Dict[str, Any]]: Reward and dict with reward terms.\n",
                "            \"\"\"\n",
                "        # Current observation\n",
                "        obs_dict = self.env.obs_dict.copy()\n",
                "\n",
                "        # Energy term\n",
                "        #reward_energy = - self.lambda_energy * obs_dict[self.energy_name]\n",
                "        reward_energy = - self.lambda_energy * obs_dict['Facility Total HVAC Electricity Demand Rate(Whole Building)']\n",
                "        # Comfort\n",
                "        comfort, temps = self._get_comfort(obs_dict)\n",
                "\n",
                "        if comfort == 0:\n",
                "            reward_comfort = 5\n",
                "        else:   \n",
                "            reward_comfort = - self.lambda_temp * comfort\n",
                "\n",
                "        # Determine energy weight depending on the hour\n",
                "        hour = obs_dict['hour']\n",
                "        if hour >= self.range_comfort_hours[0] and hour <= self.range_comfort_hours[1]:\n",
                "            weight = self.W_energy\n",
                "        else:\n",
                "            weight = 1\n",
                "\n",
                "\n",
                "        # Weighted sum of both terms\n",
                "        reward = weight * reward_energy + (1.0 - weight) * reward_comfort\n",
                "\n",
                "        reward_terms = {\n",
                "            'reward_energy': reward_energy,\n",
                "            'total_energy': obs_dict[self.energy_name],\n",
                "            'reward_comfort': reward_comfort,\n",
                "            'temperatures': temps\n",
                "        }\n",
                "\n",
                "        return reward, reward_terms\n",
                "\n",
                "class MyHourlyLinearReward(MyLinearReward):\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        env: Env,\n",
                "        temperature_variable: Union[str, list],\n",
                "        energy_variable: str,\n",
                "        range_comfort_winter: Tuple[int, int],\n",
                "        range_comfort_summer: Tuple[int, int],\n",
                "        summer_start: Tuple[int, int] = (6, 1),\n",
                "        summer_final: Tuple[int, int] = (9, 30),\n",
                "        min_energy_weight: float = 0.5,\n",
                "        lambda_energy: float = 0.005,\n",
                "        lambda_temperature: float = 100,\n",
                "        range_comfort_hours: tuple = (8, 19),\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Linear reward function with a time-dependent weight for consumption and energy terms.\n",
                "\n",
                "        Args:\n",
                "            env (Env): Gym environment.\n",
                "            temperature_variable (Union[str, list]): Name(s) of the temperature variable(s).\n",
                "            energy_variable (str): Name of the energy/power variable.\n",
                "            range_comfort_winter (Tuple[int,int]): Temperature comfort range for cold season. Depends on environment you are using.\n",
                "            range_comfort_summer (Tuple[int,int]): Temperature comfort range for hot season. Depends on environment you are using.\n",
                "            summer_start (Tuple[int,int]): Summer session tuple with month and day start. Defaults to (6,1).\n",
                "            summer_final (Tuple[int,int]): Summer session tuple with month and day end. defaults to (9,30).\n",
                "            min_energy_weight (float, optional): Minimum weight given to the energy term. Defaults to 0.5.\n",
                "            lambda_energy (float, optional): Constant for removing dimensions from power(1/W). Defaults to 1e-4.\n",
                "            lambda_temperature (float, optional): Constant for removing dimensions from temperature(1/C). Defaults to 1.0.\n",
                "            range_comfort_hours (tuple, optional): Hours where thermal comfort is considered. Defaults to (9, 19).\n",
                "        \"\"\"\n",
                "\n",
                "        super(MyHourlyLinearReward, self).__init__(\n",
                "            env,\n",
                "            temperature_variable,\n",
                "            energy_variable,\n",
                "            range_comfort_winter,\n",
                "            range_comfort_summer,\n",
                "            summer_start,\n",
                "            summer_final,\n",
                "            min_energy_weight,\n",
                "            lambda_energy,\n",
                "            lambda_temperature\n",
                "        )\n",
                "\n",
                "        # Reward parameters\n",
                "        self.range_comfort_hours = range_comfort_hours\n",
                "\n",
                "    def __call__(self) -> Tuple[float, Dict[str, Any]]:\n",
                "        \"\"\"Calculate the reward function.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, Dict[str, Any]]: Reward and dict with reward terms.\n",
                "            \"\"\"\n",
                "        # Current observation\n",
                "        obs_dict = self.env.obs_dict.copy()\n",
                "\n",
                "        # Energy term\n",
                "        reward_energy = - self.lambda_energy * obs_dict[self.energy_name]\n",
                "\n",
                "        # Comfort\n",
                "        comfort, temps = self._get_comfort(obs_dict)\n",
                "        reward_comfort = - self.lambda_temp * comfort\n",
                "\n",
                "        # Determine energy weight depending on the hour\n",
                "        hour = obs_dict['hour']\n",
                "        if hour >= self.range_comfort_hours[0] and hour <= self.range_comfort_hours[1]:\n",
                "            weight = self.W_energy\n",
                "        else:\n",
                "            weight = 1.0\n",
                "\n",
                "        # Weighted sum of both terms\n",
                "        reward = weight * reward_energy + (1.0 - weight) * reward_comfort\n",
                "\n",
                "        reward_terms = {\n",
                "            'reward_energy': reward_energy,\n",
                "            'total_energy': obs_dict[self.energy_name],\n",
                "            'reward_comfort': reward_comfort,\n",
                "            'temperatures': temps\n",
                "        }\n",
                "\n",
                "        return reward, reward_terms\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "ename": "IndexError",
                    "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [16], line 28\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# register run name\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# name = F\"{environment}-episodes_{episodes}({experiment_date})\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[39m# Set to one month only to reduce running time\u001b[39;00m\n\u001b[1;32m     21\u001b[0m extra_params\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mtimesteps_per_hour\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m4\u001b[39m,\n\u001b[1;32m     22\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mrunperiod\u001b[39m\u001b[39m'\u001b[39m : (begin_day,begin_month,begin_year,end_day,end_month,end_year)}\n\u001b[1;32m     24\u001b[0m new_observation_variables\u001b[39m=\u001b[39m[\n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSite Outdoor Air Drybulb Temperature(Environment)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mZone Air Temperature(SPACE1-1)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFacility Total HVAC Electricity Demand Rate(Whole Building)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m---> 28\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbins\u001b[39m\u001b[39m'\u001b[39m \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdigitize(obs[\u001b[39m\"\u001b[39;49m\u001b[39mZoneZone Air Temperature(SPACE1-1) \u001b[39;49m\u001b[39m\"\u001b[39;49m], bins)\n\u001b[1;32m     29\u001b[0m  ]\n\u001b[1;32m     31\u001b[0m new_observation_space \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox(\n\u001b[1;32m     32\u001b[0m     low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m5e6\u001b[39m,\n\u001b[1;32m     33\u001b[0m     high\u001b[39m=\u001b[39m\u001b[39m5e6\u001b[39m,\n\u001b[1;32m     34\u001b[0m     shape\u001b[39m=\u001b[39m(\u001b[39mlen\u001b[39m(new_observation_variables) \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m,),\n\u001b[1;32m     35\u001b[0m     dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     37\u001b[0m new_action_variables \u001b[39m=\u001b[39m [\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHeating_Setpoint_RL\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCooling_Setpoint_RL\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m ]\n",
                        "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
                    ]
                }
            ],
            "source": [
                "environment = \"Eplus-5Zone-hot-discrete-v1\"\n",
                "weather = \"USA_NY_New.York-J.F.Kennedy.Intl.AP.744860_TMY3.epw\"\n",
                "\n",
                "episodes = 1\n",
                "\n",
                "bins = np.linspace(15.125, 30.125, 61)\n",
                "\n",
                "#choose the simulation period\n",
                "begin_day = 1\n",
                "begin_month = 1\n",
                "begin_year = 2022\n",
                "end_day = 1\n",
                "end_month = 2\n",
                "end_year = 2022\n",
                "\n",
                "# register run name\n",
                "# name = F\"{environment}-episodes_{episodes}({experiment_date})\"\n",
                "\n",
                "\n",
                "# Set to one month only to reduce running time\n",
                "extra_params={'timesteps_per_hour' : 4,\n",
                "              'runperiod' : (begin_day,begin_month,begin_year,end_day,end_month,end_year)}\n",
                "\n",
                "new_observation_variables=[\n",
                "    'Site Outdoor Air Drybulb Temperature(Environment)',\n",
                "    'Zone Air Temperature(SPACE1-1)',\n",
                "    'Facility Total HVAC Electricity Demand Rate(Whole Building)',\n",
                "  #  'bins' == np.digitize(obs[\"ZoneZone Air Temperature(SPACE1-1) \"], bins)\n",
                " ]\n",
                "\n",
                "new_observation_space = gym.spaces.Box(\n",
                "    low=-5e6,\n",
                "    high=5e6,\n",
                "    shape=(len(new_observation_variables) + 4,),\n",
                "    dtype=np.float32)\n",
                "\n",
                "new_action_variables = [\n",
                "    'Heating_Setpoint_RL',\n",
                "    'Cooling_Setpoint_RL',\n",
                "]\n",
                "\n",
                "new_action_mapping = {\n",
                "    0: (15, 30),\n",
                "    1: (16, 29),\n",
                "    2: (17, 28),\n",
                "    3: (18, 27),\n",
                "    4: (19, 26),\n",
                "    5: (20, 25),\n",
                "    6: (21, 24),\n",
                "    7: (22, 23),\n",
                "    8: (22, 22),\n",
                "    9: (21, 21)\n",
                "}\n",
                "\n",
                "new_action_space = gym.spaces.Discrete(10)\n",
                "\n",
                "env = gym.make(environment, \n",
                "                weather_file = weather,\n",
                "                reward = MyHourlyExpReward, \n",
                "                config_params = extra_params,\n",
                "                observation_variables = new_observation_variables,\n",
                "                observation_space = new_observation_space,\n",
                "                action_variables=new_action_variables,\n",
                "                action_mapping=new_action_mapping,\n",
                "                action_space=new_action_space\n",
                "                )\n",
                "\n",
                "#env = LoggerWrapper(NormalizeObservation(env, ranges = RANGES_5ZONE))\n",
                "env = CSVLogger(env)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-02-08 08:52:48,441] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-02-08 08:52:48,441] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-02-08 08:52:48,441] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-02-08 08:52:48,441] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
                        "[2023-02-08 08:52:48,779] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-discrete-v1-res10/Eplus-env-sub_run1\n",
                        "[2023-02-08 08:52:48,779] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-discrete-v1-res10/Eplus-env-sub_run1\n",
                        "[2023-02-08 08:52:48,779] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-discrete-v1-res10/Eplus-env-sub_run1\n",
                        "[2023-02-08 08:52:48,779] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-5Zone-hot-discrete-v1-res10/Eplus-env-sub_run1\n",
                        "Reward:  -20.29371309771611 {'timestep': 1, 'time_elapsed': 900, 'year': 2022, 'month': 1, 'day': 1, 'hour': 0, 'total_power': 20293.71309771611, 'total_power_no_units': -20.29371309771611, 'comfort_penalty': 5, 'abs_comfort': None, 'temperatures': [20.99998854258536], 'out_temperature': -9.049999999999999, 'action_': [21, 24]}\n",
                        "Reward:  -19790.219939704126 {'timestep': 2976, 'time_elapsed': 2678400, 'year': 2022, 'month': 2, 'day': 1, 'hour': 0, 'total_power': 3302.950052087728, 'total_power_no_units': -3.3029500520877284, 'comfort_penalty': 5, 'abs_comfort': None, 'temperatures': [18.26459102113768], 'out_temperature': -1.2, 'action_': [16, 29]}\n"
                    ]
                }
            ],
            "source": [
                "for i in range(1):\n",
                "    obs = env.reset()\n",
                "    rewards = []\n",
                "    done = False\n",
                "    current_month = 0\n",
                "    while not done:\n",
                "        a = env.action_space.sample()\n",
                "        obs, reward, done, info = env.step(a)\n",
                "        rewards.append(reward)\n",
                "        if info['month'] != current_month:  # display results every month\n",
                "            current_month = info['month']\n",
                "            print('Reward: ', sum(rewards), info)\n",
                "        if current_month == 2:\n",
                "            done = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
                        "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
                        "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
                        "  ret = ret.dtype.type(ret / rcount)\n",
                        "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
                        "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
                        "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
                        "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
                        "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
                        "  ret = ret.dtype.type(ret / rcount)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2023-02-08 08:54:51,296] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n",
                        "[2023-02-08 08:54:51,296] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n",
                        "[2023-02-08 08:54:51,296] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n",
                        "[2023-02-08 08:54:51,296] EPLUS_ENV_5Zone-hot-discrete-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n"
                    ]
                }
            ],
            "source": [
                "env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Getting the state space\n",
                "print(\"Action Space {}\".format(env.action_space))\n",
                "print(\"State Space {}\".format(env.observation_space))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'Box' object is not callable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space(\u001b[39m\"\u001b[39;49m\u001b[39mhour\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'Box' object is not callable"
                    ]
                }
            ],
            "source": [
                "env.observation_space(\"hour\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#discrete observation space size (make bins)\n",
                "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
                "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(DISCRETE_OS_SIZE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "q_table = np.zeros(shape=DISCRETE_OS_SIZE+[env.action_space.n])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(len(q_table[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# env = gym.make(\"MountainCar-v0\")\n",
                "# env.reset()\n",
                "\n",
                "# done = False\n",
                "\n",
                "# while not done:\n",
                "#     action = 2\n",
                "#     new_state, reward, done, _ = env.step(action)\n",
                "#     env.render()\n",
                "    \n",
                "# env.close()\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_actions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hours = np.arange(0, 24, 1)\n",
                "temp = np.arange(15, 30, 0.1)\n",
                "temp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Initialize the Q-table to 0\n",
                "# Q_table = np.zeros(len(hours),len(temp))\n",
                "Q_table = np.zeros(n_obsservations,len(temp))\n",
                "print(Q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#number of episode we will run\n",
                "n_episodes = 10000\n",
                "\n",
                "#maximum of iteration per episode\n",
                "max_iter_episode = 100\n",
                "\n",
                "#initialize the exploration probability to 1\n",
                "exploration_proba = 1\n",
                "\n",
                "#exploartion decreasing decay for exponential decreasing\n",
                "exploration_decreasing_decay = 0.001\n",
                "\n",
                "# minimum of exploration proba\n",
                "min_exploration_proba = 0.01\n",
                "\n",
                "#discounted factor\n",
                "gamma = 0.99\n",
                "\n",
                "#learning rate\n",
                "lr = 0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_rewards_episode = list()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#we iterate over episodes\n",
                "for e in range(n_episodes):\n",
                "    #we initialize the first state of the episode\n",
                "    current_state = env.reset()\n",
                "    done = False\n",
                "    \n",
                "    #sum the rewards that the agent gets from the environment\n",
                "    total_episode_reward = 0\n",
                "    \n",
                "    for i in range(max_iter_episode): \n",
                "        # we sample a float from a uniform distribution over 0 and 1\n",
                "        # if the sampled float is less than the exploration probability\n",
                "        #     the agent selects a random action\n",
                "        # else\n",
                "        #     he exploits his knowledge using the bellman equation \n",
                "        \n",
                "        if np.random.uniform(0,1) < exploration_proba:\n",
                "            action = env.action_space.sample()\n",
                "        else:\n",
                "            action = np.argmax(Q_table[current_state,:])\n",
                "        \n",
                "        # The environment runs the chosen action and returns\n",
                "        # the next state, a reward and true if the episode is ended.\n",
                "        next_state, reward, done, _ = env.step(action)\n",
                "        \n",
                "        # We update our Q-table using the Q-learning iteration\n",
                "        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
                "        total_episode_reward = total_episode_reward + reward\n",
                "        # If the episode is finished, we leave the for loop\n",
                "        if done:\n",
                "            break\n",
                "        current_state = next_state\n",
                "    #We update the exploration proba using exponential decay formula \n",
                "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
                "    total_rewards_episode.append(total_episode_reward)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Mean reward per thousand episodes\")\n",
                "for i in range(10):\n",
                "    print((i+1)*1000,\": mean espiode reward: \",\\\n",
                "           np.mean(total_rewards_episode[1000*i:1000*(i+1)]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "make 2 observation variables discrete.\n",
                "For example:\n",
                "Outside temp\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sinergym\n",
                "from sinergym.utils.callbacks import LoggerEvalCallback\n",
                "from sinergym.utils.rewards import *\n",
                "from sinergym.utils.wrappers import LoggerWrapper\n",
                "from datetime import datetime\n",
                "import gym\n",
                "from stable_baselines3 import DQN, DDPG, PPO, A2C, SAC, TD3 \n",
                "\n",
                "from stable_baselines3.common.callbacks import CallbackList\n",
                "from stable_baselines3.common.vec_env import DummyVecEnv\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "environment  = \"Eplus-5Zone-hot-continuous-v1\"\n",
                "weather = \"USA_NY_New.York-J.F.Kennedy.Intl.AP.744860_TMY3.epw\"\n",
                "\n",
                "episodes = 3\n",
                "experiment_date = datetime.today().strftime('%Y-%m-%d %H:%M')\n",
                "\n",
                "#choose the simulation period\n",
                "begin_day = 1\n",
                "begin_month = 1\n",
                "begin_year = 2022\n",
                "end_day = 1\n",
                "end_month = 2\n",
                "end_year = 2022\n",
                "\n",
                "# register run name\n",
                "name = F\"{environment}-episodes_{episodes}({experiment_date})\"\n",
                "\n",
                "\n",
                "# Set to one month only to reduce running time\n",
                "extra_params={'timesteps_per_hour' : 4,\n",
                "              'runperiod' : (begin_day,begin_month,begin_year,end_day,end_month,end_year)}\n",
                "\n",
                "new_observation_variables=[\n",
                "    'Site Outdoor Air Drybulb Temperature(Environment)',\n",
                "    'Site Diffuse Solar Radiation Rate per Area(Environment)',\n",
                "    'Site Direct Solar Radiation Rate per Area(Environment)',\n",
                "    'Zone Thermostat Heating Setpoint Temperature(SPACE1-1)',\n",
                "    'Zone Thermostat Cooling Setpoint Temperature(SPACE1-1)',\n",
                "    'Zone Air Temperature(SPACE1-1)',\n",
                "    'Zone People Occupant Count(SPACE1-1)',\n",
                "    'Facility Total HVAC Electricity Demand Rate(Whole Building)']\n",
                "\n",
                "new_observation_space = gym.spaces.Box(\n",
                "    low=-5e6,\n",
                "    high=5e6,\n",
                "    shape=(len(new_observation_variables) + 4,),\n",
                "    dtype=np.float32)\n",
                "\n",
                "\n",
                "env = gym.make(environment, \n",
                "                weather_file = weather,\n",
                "                reward = ExpReward, \n",
                "                config_params = extra_params,\n",
                "                observation_variables = new_observation_variables,\n",
                "                observation_space = new_observation_space,\n",
                "                reward_kwargs={\n",
                "                  'temperature_variable': 'Zone Air Temperature (SPACE1-1)',\n",
                "                    'energy_variable': 'Facility Total HVAC Electricity Demand Rate(Whole Building)',\n",
                "                    'range_comfort_winter': (20.0, 23.5),\n",
                "                    'range_comfort_summer': (23.0, 26.0),\n",
                "                    'energy_weight': 0.5 })\n",
                "\n",
                "\n",
                "env = LoggerWrapper(env)\n",
                "\n",
                "\n",
                "model = PPO('MlpPolicy', env, verbose=1,\n",
                "        learning_rate = 0.001)\n",
                "\n",
                "n_timesteps_episode = env.simulator._eplus_one_epi_len / \\\n",
                "                      env.simulator._eplus_run_stepsize\n",
                "\n",
                "env_vec = DummyVecEnv([lambda: env])\n",
                "\n",
                "callbacks = []\n",
                "\n",
                "# Set up Evaluation and saving best model\n",
                "eval_callback = LoggerEvalCallback(\n",
                "    env_vec,\n",
                "    best_model_save_path='best_model/' + name + '/',\n",
                "    log_path='best_model/' + name + '/',\n",
                "    eval_freq=n_timesteps_episode * 2,\n",
                "    deterministic=True,\n",
                "    render=False,\n",
                "    n_eval_episodes=2)\n",
                "callbacks.append(eval_callback)\n",
                "\n",
                "callback = CallbackList(callbacks)\n",
                "\n",
                "timesteps = episodes * n_timesteps_episode\n",
                "\n",
                "model.learn(\n",
                "    total_timesteps=timesteps,\n",
                "    callback=callback,\n",
                "    log_interval=1)\n",
                "\n",
                "model.save(env.simulator._env_working_dir_parent + '/' + name)\n",
                "\n",
                "env.close()\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.6 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
